{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T18:32:00.889040Z",
     "iopub.status.busy": "2024-12-26T18:32:00.888636Z",
     "iopub.status.idle": "2024-12-26T18:33:53.794959Z",
     "shell.execute_reply": "2024-12-26T18:33:53.794295Z",
     "shell.execute_reply.started": "2024-12-26T18:32:00.889002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training samples: 36866, Validation samples: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n",
      "100%|██████████| 74.5M/74.5M [00:00<00:00, 205MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "base_path = '/kaggle/input/'\n",
    "train_csv_path = os.path.join(base_path, 'project/train.csv')\n",
    "test_csv_path = os.path.join(base_path, 'project/test.csv')\n",
    "local_train_dir = base_path + 'images/train_data/train_data'\n",
    "local_test_dir = base_path + 'images/test_data/test_data'\n",
    "\n",
    "# Load and filter training CSV\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_df['labels'] = train_df['labels'].apply(lambda x: x.split(', '))\n",
    "\n",
    "def file_exists_in_local_dir(filename, folder):\n",
    "    return os.path.exists(os.path.join(folder, filename))\n",
    "\n",
    "train_df = train_df[train_df['filename'].apply(lambda x: file_exists_in_local_dir(x, local_train_dir))]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_df['labels'])\n",
    "num_classes = len(mlb.classes_)\n",
    "\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, df, img_folder, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.img_folder = img_folder\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label_vec = self.labels[idx]\n",
    "            return image, torch.tensor(label_vec, dtype=torch.float32)\n",
    "        else:\n",
    "            return image, img_name\n",
    "\n",
    "# Basic data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset = MultiLabelDataset(train_df, local_train_dir, labels=train_labels, transform=transform)\n",
    "validation_size = 1000\n",
    "train_size = len(train_dataset) - validation_size\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 測試集保持不變\n",
    "test_dataset = MultiLabelDataset(test_df, local_test_dir, labels=None, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Validation samples: {validation_size}\")\n",
    "\n",
    "\n",
    "# Load EfficientNet-B0 model with pretrained weights\n",
    "weights = EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "model = efficientnet_b4(weights=weights)\n",
    "\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Focal loss for imbalanced datasets\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, epochs=num_epochs, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T18:34:29.541759Z",
     "iopub.status.busy": "2024-12-26T18:34:29.541445Z",
     "iopub.status.idle": "2024-12-26T20:54:51.983216Z",
     "shell.execute_reply": "2024-12-26T20:54:51.981778Z",
     "shell.execute_reply.started": "2024-12-26T18:34:29.541736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1153/1153 [20:10<00:00,  1.05s/it, Batch Loss=0.0269] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0132\n",
      "Epoch [1/10], mAP: 0.6133\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1153/1153 [16:52<00:00,  1.14it/s, Batch Loss=0.0229] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0114\n",
      "Epoch [2/10], mAP: 0.6176\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1153/1153 [16:51<00:00,  1.14it/s, Batch Loss=0.0192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0099\n",
      "Epoch [3/10], mAP: 0.6590\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1153/1153 [16:52<00:00,  1.14it/s, Batch Loss=0.016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0087\n",
      "Epoch [4/10], mAP: 0.6743\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1153/1153 [16:49<00:00,  1.14it/s, Batch Loss=0.0439] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0077\n",
      "Epoch [5/10], mAP: 0.6777\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1153/1153 [16:56<00:00,  1.13it/s, Batch Loss=0.016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0069\n",
      "Epoch [6/10], mAP: 0.7000\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1153/1153 [17:02<00:00,  1.13it/s, Batch Loss=0.00849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0062\n",
      "Epoch [7/10], mAP: 0.7028\n",
      "New best model saved to /kaggle/working/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1153/1153 [16:47<00:00,  1.14it/s, Batch Loss=0.0167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0055\n",
      "Epoch [8/10], mAP: 0.6920\n",
      "No improvement in mAP. Early stop counter: 1/1\n",
      "Early stopping triggered. Best mAP: 0.7028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to evaluate mAP\n",
    "def evaluate_mAP(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "\n",
    "    # Calculate average precision for each class\n",
    "    ap_per_class = []\n",
    "    for i in range(num_classes):\n",
    "        if np.sum(all_targets[:, i]) == 0:\n",
    "            continue  # Skip classes with no positive samples\n",
    "        ap = average_precision_score(all_targets[:, i], all_predictions[:, i])\n",
    "        ap_per_class.append(ap)\n",
    "\n",
    "    # Compute mAP\n",
    "    mAP = np.mean(ap_per_class)\n",
    "    return mAP\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 1  # Number of epochs to wait for improvement\n",
    "best_mAP = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  # Normal criterion without Mixup\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate mAP on test data\n",
    "    mAP = evaluate_mAP(model, validation_loader, device, num_classes)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], mAP: {mAP:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if mAP > best_mAP:\n",
    "        best_mAP = mAP\n",
    "        early_stop_counter = 0\n",
    "        # Save the best model\n",
    "        best_model_path = f'/kaggle/working/best_model.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'mAP': mAP,\n",
    "        }, best_model_path)\n",
    "        print(f\"New best model saved to {best_model_path}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"No improvement in mAP. Early stop counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "    # Check if early stopping condition is met\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Best mAP: {best_mAP:.4f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T20:57:21.570128Z",
     "iopub.status.busy": "2024-12-26T20:57:21.569806Z",
     "iopub.status.idle": "2024-12-26T21:00:18.319989Z",
     "shell.execute_reply": "2024-12-26T21:00:18.319355Z",
     "shell.execute_reply.started": "2024-12-26T20:57:21.570102Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, filenames in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = torch.sigmoid(model(images))\n",
    "        outputs = outputs.cpu().numpy()\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            probs = outputs[i, :num_classes]\n",
    "            results.append([filename] + probs.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T21:00:18.321165Z",
     "iopub.status.busy": "2024-12-26T21:00:18.320924Z",
     "iopub.status.idle": "2024-12-26T21:00:19.326090Z",
     "shell.execute_reply": "2024-12-26T21:00:19.325386Z",
     "shell.execute_reply.started": "2024-12-26T21:00:18.321141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to: /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "columns = ['filename'] + [f'class_{i}_prob' for i in range(num_classes)]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns_to_keep = ['filename'] + [f'class_{i}_prob' for i in range(79)]\n",
    "submission_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Define the output path (use /kaggle/working if in a Kaggle environment)\n",
    "submission_file = os.path.join('/kaggle/working', 'submission.csv')\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Submission file saved to: {submission_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是要讀取已經有紀錄起來的 model 改路徑即可使用 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T18:34:04.695795Z",
     "iopub.status.busy": "2024-12-26T18:34:04.695510Z",
     "iopub.status.idle": "2024-12-26T18:34:07.479617Z",
     "shell.execute_reply": "2024-12-26T18:34:07.478861Z",
     "shell.execute_reply.started": "2024-12-26T18:34:04.695773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從 /kaggle/input/model-for-sldl/pytorch/default/1/best_model.pth 載入最佳模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-337cfe63d9f9>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從 epoch 3 繼續訓練，最佳 mAP: 0.5171\n"
     ]
    }
   ],
   "source": [
    "# 設定模型文件的路徑\n",
    "model_path = '/kaggle/input/model-for-sldl/pytorch/default/1/best_model.pth'  # 根據實際文件名稱設置\n",
    "\n",
    "# 檢查該路徑是否存在\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"從 {model_path} 載入最佳模型\")\n",
    "    \n",
    "    # 載入模型檔案\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # 載入模型和優化器的 state_dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_mAP = checkpoint['mAP']  # 恢復最佳 mAP\n",
    "    \n",
    "    print(f\"從 epoch {epoch+1} 繼續訓練，最佳 mAP: {best_mAP:.4f}\")\n",
    "else:\n",
    "    print(f\"沒有找到模型檔案，從頭開始訓練。\")\n",
    "    epoch = 0  # 從第一個 epoch 開始\n",
    "    best_mAP = 0.0  # 尚未有最佳模型\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6373893,
     "sourceId": 10297862,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6373930,
     "sourceId": 10297919,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 202065,
     "modelInstanceId": 179798,
     "sourceId": 210898,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
